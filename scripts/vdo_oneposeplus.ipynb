{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import natsort\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_video(image_path, output_video_path):\n",
    "#     # Generate video:\n",
    "#     images = natsort.natsorted(os.listdir(image_path))\n",
    "#     # Path(output_video_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "#     print(images[0])\n",
    "#     H, W, C = cv2.imread(os.path.join(image_path,images[0])).shape\n",
    "#     # if Path(output_video_path).exists():\n",
    "#     #     Path(output_video_path).unlink()\n",
    "    \n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     video = cv2.VideoWriter(f'{output_video_path}\\\\inference', fourcc, 24, (W, H))\n",
    "#     for id, image_name in enumerate(images):\n",
    "#         # image = cv2.imread(str(Path(image_path) / image_name))\n",
    "#         image = cv2.imread(os.path.join(image_path,image_name))\n",
    "#         video.write(image)\n",
    "#     video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_video(image_path, output_video_path, output_filename=\"inference.mp4\", fps=24):\n",
    "    # Ensure output directory exists\n",
    "    Path(output_video_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get sorted list of image files\n",
    "    images = natsort.natsorted(os.listdir(image_path))\n",
    "\n",
    "    if not images:\n",
    "        raise ValueError(\"No images found in the provided directory.\")\n",
    "\n",
    "    # Read first image to get dimensions\n",
    "    first_image_path = os.path.join(image_path, images[0])\n",
    "    first_image = cv2.imread(first_image_path)\n",
    "    if first_image is None:\n",
    "        raise ValueError(f\"Could not read the first image: {first_image_path}\")\n",
    "    H, W, _ = first_image.shape\n",
    "\n",
    "    # Create VideoWriter object\n",
    "    output_file = os.path.join(output_video_path, output_filename)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_file, fourcc, fps, (W, H))\n",
    "\n",
    "    for image_name in images:\n",
    "        image_path_full = os.path.join(image_path, image_name)\n",
    "        image = cv2.imread(image_path_full)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Skipping unreadable file {image_path_full}\")\n",
    "            continue\n",
    "        video.write(image)\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Video saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_and_poses(img_id , cropped=True):\n",
    "    img_inf = f'color\\\\{img_id}.png'\n",
    "    img_uncrop = f'D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\color_full\\\\{img_id}.jpg'\n",
    "    pred_path = f'D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\predicted_pose\\\\{img_id}.txt' \n",
    "    gt_path = f'./oneposeplus_inference/small_set/poses_ba/{img_id}.txt'\n",
    "    k_path = f'intrin_ba\\\\{img_id}.txt'\n",
    "\n",
    "    #Original Intrinsic \n",
    "    fx,fy,cx,cy = 1400.00, 1400, 1920, 1080\n",
    "\n",
    "    K = np.array([[fx,0,cx],\n",
    "                    [0,fy,cy],\n",
    "                        [0,0,1]])\n",
    "\n",
    "    pose = np.loadtxt(pred_path)\n",
    "    # pose_gt = np.loadtxt(gt_path)\n",
    "    # K_cropped = np.loadtxt(k_path)\n",
    "    if cropped :\n",
    "        return img_inf, pose # pose_gt, K_cropped\n",
    "    else:\n",
    "        return img_uncrop, pose,  K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection(points_3D, transformation, internal_calibration):\n",
    "    projections_2d = np.zeros((2, points_3D.shape[1]), dtype='float32')\n",
    "    camera_projection = (internal_calibration.dot(transformation)).dot(points_3D)\n",
    "    # camera_projection = (internal_calibration@(transformation))@(points_3D)\n",
    "    projections_2d[0, :] = camera_projection[0, :] / camera_projection[2, :]\n",
    "    projections_2d[1, :] = camera_projection[1, :] / camera_projection[2, :]\n",
    "    return projections_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_demo_img_corners(img, projectpts, color = (0, 255, 0), nV=8, thickness = 5):\n",
    "\n",
    "    vertices = []\n",
    "    for i in range(nV):\n",
    "        x = projectpts[i][0]\n",
    "        y = projectpts[i][1]\n",
    "        coordinates = (int(x), int(y))\n",
    "        vertices.append(coordinates)\n",
    "        cv2.circle(img, coordinates, 2, color, -1)\n",
    "\n",
    "    # print(vertices)\n",
    "    cv2.line(img, vertices[0], vertices[1], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[0], vertices[2], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[0], vertices[4], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[1], vertices[5], color, thickness =  thickness)\n",
    "    cv2.line(img, vertices[1], vertices[3], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[2], vertices[3], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[2], vertices[6], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[3], vertices[7], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[4], vertices[5], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[4], vertices[6], color, thickness = thickness)\n",
    "    cv2.line(img, vertices[5], vertices[7], color, thickness =  thickness)\n",
    "    cv2.line(img, vertices[6], vertices[7], color, thickness = thickness)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Docking Station\n",
    "# min_x,min_y,min_z = -310.211914,-626.390503,-617.810913\n",
    "# max_x,max_y,max_z = 310.211914,626.390503,617.810913\n",
    "# corners = np.array([[min_x, min_y, min_z],\n",
    "#                     [min_x, min_y, max_z],\n",
    "#                     [min_x, max_y, min_z],\n",
    "#                     [min_x, max_y, max_z],\n",
    "#                     [max_x, min_y, min_z],\n",
    "#                     [max_x, min_y, max_z],\n",
    "#                     [max_x, max_y, min_z],\n",
    "#                     [max_x, max_y, max_z]])\n",
    "# corners = np.concatenate((np.transpose(corners), np.ones((1, 8))), axis=0)\n",
    "# corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Inc_shell\n",
    "# min_x,min_y,min_z = -2.1459435,-0.6427165,-0.699005\n",
    "# max_x,max_y,max_z =  2.1459435,  0.6427165,  0.699005\n",
    "# corners = np.array([[min_x, min_y, min_z],\n",
    "#                     [min_x, min_y, max_z],\n",
    "#                     [min_x, max_y, min_z],\n",
    "#                     [min_x, max_y, max_z],\n",
    "#                     [max_x, min_y, min_z],\n",
    "#                     [max_x, min_y, max_z],\n",
    "#                     [max_x, max_y, min_z],\n",
    "#                     [max_x, max_y, max_z]])\n",
    "# corners = np.concatenate((np.transpose(corners), np.ones((1, 8))), axis=0)\n",
    "# corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.627172 , -1.627172 , -1.627172 , -1.627172 ,  1.627172 ,\n",
       "         1.627172 ,  1.627172 ,  1.627172 ],\n",
       "       [-0.4971785, -0.4971785,  0.4971785,  0.4971785, -0.4971785,\n",
       "        -0.4971785,  0.4971785,  0.4971785],\n",
       "       [-0.5440315,  0.5440315, -0.5440315,  0.5440315, -0.5440315,\n",
       "         0.5440315, -0.5440315,  0.5440315],\n",
       "       [ 1.       ,  1.       ,  1.       ,  1.       ,  1.       ,\n",
       "         1.       ,  1.       ,  1.       ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#500lb\n",
    "min_x,min_y,min_z = -1.627172, -0.4971785, -0.5440315\n",
    "max_x,max_y,max_z =  1.627172,  0.4971785,  0.5440315\n",
    "corners = np.array([[min_x, min_y, min_z],\n",
    "                    [min_x, min_y, max_z],\n",
    "                    [min_x, max_y, min_z],\n",
    "                    [min_x, max_y, max_z],\n",
    "                    [max_x, min_y, min_z],\n",
    "                    [max_x, min_y, max_z],\n",
    "                    [max_x, max_y, min_z],\n",
    "                    [max_x, max_y, max_z]])\n",
    "corners = np.concatenate((np.transpose(corners), np.ones((1, 8))), axis=0)\n",
    "corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_gt = np.loadtxt(pose_file)\n",
    "# R = rotation_180_z@np.array(pose_gt)[:3,:3]\n",
    "# T = np.array(pose_gt)[:3,3:]* 1000\n",
    "# pose  = np.concatenate((R, T), 1)\n",
    "# projections = compute_projection(corners,pose,K)\n",
    "# print(projections)\n",
    "# corners2D_pr = np.transpose(projections) #add .flatten() for saving annotations\n",
    "# flat = corners2D_pr.flatten().astype(float)\n",
    "# print(corners2D_pr)\n",
    "# print(flat)\n",
    "# print(projections.flatten())\n",
    "# # print(projections[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_ori = cv2.imread(image_file)\n",
    "# # print(img_ori)\n",
    "# img_ori = draw_demo_img_corners(img_ori,flat.reshape(-1, 2), (0, 0, 255), nV=8)\n",
    "# # cv2.imwrite(\"data\\\\deepurl_docking_station\\\\ex_box3.jpg\", img_ori)\n",
    "# cv2.imshow('Image', img_ori)\n",
    "# k = cv2.waitKey(0) & 0XFF\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project point and save video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = 'D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\predicted_pose' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "#Plot boxes and save image\n",
    "# i = 0\n",
    "pose_files = os.listdir(pose_path)\n",
    "num_images = len(pose_files)\n",
    "print(num_images)\n",
    "for idx in range(num_images):\n",
    "    # image_width, image_height = 2048,1536\n",
    "    # # YOLOv8 detection output: [class_id, x_center, y_center, width, height, confidence]\n",
    "    # yolo_detection = np.loadtxt(os.path.join(yolo_path,f'{'.'.join(image.split('.')[0:2])}.txt'))  # Example\n",
    "\n",
    "    # # Convert YOLO format to (xmin, ymin, xmax, ymax)\n",
    "    # x_center, y_center, width, height = yolo_detection[1:5]\n",
    "    # xmin = int((x_center - width / 2) * image_width)\n",
    "    # ymin = int((y_center - height / 2) * image_height)\n",
    "    # xmax = int((x_center + width / 2) * image_width)\n",
    "    # ymax = int((y_center + height / 2) * image_height)\n",
    "    # bbox = [xmin, ymin, xmax, ymax]\n",
    "    # bbox = list(map(float, bbox))\n",
    "    # print(bbox)\n",
    "    filename, pose, K  = get_path_and_poses(idx , cropped=False)\n",
    "    R = np.array(pose)[:3,:3]\n",
    "    T = np.array(pose)[:3,3:] #*1000 for docking station\n",
    "    pose  = np.concatenate((R, T), 1)\n",
    "    projections = compute_projection(corners,pose,K)\n",
    "    keypoints = np.transpose(projections)\n",
    "\n",
    "    img_ori = cv2.imread(filename)\n",
    "    # print(img_ori)\n",
    "    # print(img_ori)\n",
    "    try: \n",
    "        img_ori = draw_demo_img_corners(img_ori, keypoints, (255, 0,0 ), nV=8)\n",
    "    except:\n",
    "        print(\"something went wrong:\",idx)\n",
    "    cv2.imwrite(f\"D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\pose_plot\\\\{idx}_box3d.jpg\", img_ori)\n",
    "\n",
    "    # line = [i, path_to_img, image_width, image_height, 0]\n",
    "    # line.extend(bbox)  # xmin, ymin, xmax, ymax\n",
    "    # line.extend(keypoints)  # x1, y1, ..., x8, y8\n",
    "    # label_line = \" \".join(map(str, line)) + \"\\n\"\n",
    "    # print(label_line)\n",
    "    # f.write(label_line)\n",
    "    \n",
    "#  with open(save_file, \"w\") as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to D:\\emjmd_mir\\norway\\thesis\\thesis_ws\\deepurl_docker\\deep_underwater_localization-1.0\\oneposeplus_inference\\uxo\\500lb\\inference_video\\inference3.mp4\n"
     ]
    }
   ],
   "source": [
    "frame_path=f\"D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\pose_plot\"\n",
    "vdo_save_path =f\"D:\\\\emjmd_mir\\\\norway\\\\thesis\\\\thesis_ws\\\\deepurl_docker\\\\deep_underwater_localization-1.0\\\\oneposeplus_inference\\\\uxo\\\\500lb\\\\inference_video\" \n",
    "make_video(frame_path, vdo_save_path,output_filename=\"inference3.mp4\",fps=30)\n",
    "#build video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose_extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
